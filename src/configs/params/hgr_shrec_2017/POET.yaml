# decouple
# random_k
# prompt_type

in_channels: &in_channels 3
seq_len: &seq_len 8
rm_global_scale: &rm_global_scale On
batch_size: 32
workers: 1
total_epochs: 300
total_epochs_incremental_task: 100
step_per_epoch: &step_per_epoch On
step_per_batch: &step_per_batch Off
log_freq:
    train: 20
    val: null

model:
    name: 'dg_sta_poet'
    in_channels: *in_channels
    n_heads: 8 # number of attention heads
    d_head: 32 # dimension of attention heads
    d_feat: 128 # feature dimension
    seq_len: *seq_len # sequence length
    n_joints: 22 # number of joints
    dropout: 0.2
    num_classes: 14
    first_split_size: 8
    # prompt specific parameters
    # prompt_length -> joints shrec 22
    # top_k -> frames shrec 8
    prompt:
        prompt_type: 7    # offset
        prompt_pool: True
        pool_size: 8
        prompt_length: 22
        top_k: 8
        decouple: False
        random_k: 2        # 是否增加 prompt
        stack_random_before: False
        freeze_prevtask_prompts: True
        sort: True
        random_expand: False
        force_select_new: True
        temporal_pe: False
        prompt_init: uniform
        prompt_key: True
        embed_dim: 128
        embed_dim_prompt: 128
        prompt_key_init: uniform
        batchwise_prompt: False
        embedding_key: query_cls
        loss_coeff: 0.1


transforms:
    train:
        random_time_interpolation:
            prob: 0.5

        stratified_sample:
            n_samples: *seq_len

        random_scale:
            lim: [0.1, 5.0] #[0.8, 1.2]

        random_translation:
            x: 0.1
            y: 0.1
            z: 0.1

        random_rotation: # degrees
            x: 5.0
            y: 5.0
            z: 5.0

        center_by_index:
            ind: 1 # palm index
            

    val: &transforms_inference
        stratified_sample:
            n_samples: *seq_len

        center_by_index:
            ind: 1 # palm index                
    testval: *transforms_inference  
    testtrain: *transforms_inference 
    test: *transforms_inference


loss:
    cross_entropy: 
        weight: 1.0
        ignore: null
        class_weight: null


optimizer:
    name: 'adam'
    lr: &lr 0.001
    lr_incremental_task: 0.01
    betas: [0.9, 0.999]
    include_scheduler: False
    scheduler:
        name: 'linear'
        lr: *lr
        start_factor: 1
        end_factor: 0.001
        step_per_batch: *step_per_batch
        step_per_epoch: *step_per_epoch
    query:
        query_lr: 0.001
        dropout: 0.2
        fc_adaptor: True
          

increm:
    load_pretrained_task0: False
    first_split_size: 8
#    other_split_size: 2
#    max_task: 4
    other_split_size: 1
    max_task: 7
    load_best_checkpoint_train: True  # Only for task_id == 1
    load_best_checkpoint_test: True # Only for task_id == 1
    freeze_feature_extractor: True
    freeze_classifier: False
    memory: 0
    learner:
        type: 'poet'
        n_samples_per_class: 300
#        n_samples_per_class: 10
        power_iters: 1000         
        deep_inv_params: [0.1, 50, 0.001, 10, 1, -0.5, 5]  
        inversion_batch_size: 128
        inverted_acc_thred: 99


